{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Installation**","metadata":{}},{"cell_type":"markdown","source":"# Translating medical terminology for rare diseases used by laypeople into the Human Phenotype Ontology with NLP and the BERT model\n\n## BlueBERT implementation for comparison","metadata":{}},{"cell_type":"markdown","source":"## Package Installation\n\nBelow is the command to install all the required packages for the project.","metadata":{}},{"cell_type":"code","source":"# Installing necessary packages\n!pip install transformers pandas numpy torch scikit-learn keras-preprocessing plotly prettytable","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:13:35.674532Z","iopub.execute_input":"2023-10-01T21:13:35.674935Z","iopub.status.idle":"2023-10-01T21:13:45.336682Z","shell.execute_reply.started":"2023-10-01T21:13:35.674901Z","shell.execute_reply":"2023-10-01T21:13:45.335554Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.23.5)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nCollecting keras-preprocessing\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (5.15.0)\nRequirement already satisfied: prettytable in /opt/conda/lib/python3.10/site-packages (3.8.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from keras-preprocessing) (1.16.0)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly) (8.2.2)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prettytable) (0.2.6)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nInstalling collected packages: keras-preprocessing\nSuccessfully installed keras-preprocessing-1.1.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Importing Required Libraries and Packages\n\nBelow are the import statements required for the project.","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries from the transformers package for BERT model\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Importing data manipulation libraries\nimport pandas as pd\nimport numpy as np\n\n# Importing torch for neural network and related operations\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Importing DataLoader and TensorDataset for batching and managing datasets in PyTorch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Importing loss functions from PyTorch for binary and multi-class classification\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n\n# Importing optimization algorithms from PyTorch\nfrom torch.optim import AdamW, SGD\n\n# Importing learning rate scheduler from PyTorch\nfrom torch.optim.lr_scheduler import StepLR\n\n# Importing train-test split function from scikit-learn for splitting datasets\nfrom sklearn.model_selection import train_test_split\n\n# Importing accuracy score from scikit-learn for model evaluation\nfrom sklearn.metrics import accuracy_score\n\n# Importing sequence padding function from Keras preprocessing\nfrom keras_preprocessing.sequence import pad_sequences\n\n# Importing various metrics from scikit-learn for model evaluation\nfrom sklearn.metrics import (\n    precision_recall_fscore_support, \n    hamming_loss, \n    jaccard_score, \n    log_loss, \n    accuracy_score, \n    roc_auc_score\n)\n\n# Importing Plotly's graph objects for data visualization\nimport plotly.graph_objects as go\n\n# Importing PrettyTable for tabular data representation\nfrom prettytable import PrettyTable","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:13:45.338769Z","iopub.execute_input":"2023-10-01T21:13:45.339779Z","iopub.status.idle":"2023-10-01T21:13:57.913673Z","shell.execute_reply.started":"2023-10-01T21:13:45.339744Z","shell.execute_reply":"2023-10-01T21:13:57.912581Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data Loading and Preliminary Analysis\n\nWe load a dataset from a specified path, display its dimensions, show a few rows of data, compute and display the maximum sentence length in a specific column, and report the total number of records in the following code snippets. Finally, the text data and labels are separated for further processing.\n\n### Note\nIf running on local or any other machine, please change the file path below.","metadata":{}},{"cell_type":"code","source":"# Load the data from a CSV file\n# The commented line is an alternative path for a different dataset\n# df = pd.read_csv('')\ndf = pd.read_csv('/kaggle/input/r-diseases-dataset/model_input_latest.csv')\n\n# Print the shape of the dataframe to get an understanding of its size\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:13:57.914989Z","iopub.execute_input":"2023-10-01T21:13:57.915355Z","iopub.status.idle":"2023-10-01T21:13:58.300730Z","shell.execute_reply.started":"2023-10-01T21:13:57.915320Z","shell.execute_reply":"2023-10-01T21:13:58.299657Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(16935, 243)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Display the first 5 rows of the data to understand its structure\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:13:58.302996Z","iopub.execute_input":"2023-10-01T21:13:58.303336Z","iopub.status.idle":"2023-10-01T21:13:58.333313Z","shell.execute_reply.started":"2023-10-01T21:13:58.303306Z","shell.execute_reply":"2023-10-01T21:13:58.332155Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                    week_description  HP:0000006  HP:0000010  \\\n0  At the start of this week, I began experiencin...           0           0   \n1  At the start of this week, I began experiencin...           0           0   \n2  The tingling and numbness I mentioned earlier ...           0           0   \n3  The tingling and numbness I mentioned earlier ...           0           0   \n4  I've started experiencing episodes of dizzines...           0           0   \n\n   HP:0000012  HP:0000019  HP:0000103  HP:0000107  HP:0000201  HP:0000211  \\\n0           0           0           0           0           0           0   \n1           0           0           0           0           0           0   \n2           0           0           0           0           0           0   \n3           0           0           0           0           0           0   \n4           0           0           0           0           0           0   \n\n   HP:0000217  ...  HP:0100651  HP:0100749  HP:0100752  HP:0100759  \\\n0           0  ...           0           0           0           0   \n1           0  ...           0           0           0           0   \n2           0  ...           0           0           0           0   \n3           0  ...           0           0           0           0   \n4           0  ...           0           0           0           0   \n\n   HP:0100760  HP:0100851  HP:0100852  HP:0100868  HP:0100891  HP:0200099  \n0           0           0           0           0           0           0  \n1           0           0           0           0           0           0  \n2           0           0           0           0           0           0  \n3           0           0           0           0           0           0  \n4           0           0           0           0           0           0  \n\n[5 rows x 243 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>week_description</th>\n      <th>HP:0000006</th>\n      <th>HP:0000010</th>\n      <th>HP:0000012</th>\n      <th>HP:0000019</th>\n      <th>HP:0000103</th>\n      <th>HP:0000107</th>\n      <th>HP:0000201</th>\n      <th>HP:0000211</th>\n      <th>HP:0000217</th>\n      <th>...</th>\n      <th>HP:0100651</th>\n      <th>HP:0100749</th>\n      <th>HP:0100752</th>\n      <th>HP:0100759</th>\n      <th>HP:0100760</th>\n      <th>HP:0100851</th>\n      <th>HP:0100852</th>\n      <th>HP:0100868</th>\n      <th>HP:0100891</th>\n      <th>HP:0200099</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>At the start of this week, I began experiencin...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>At the start of this week, I began experiencin...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The tingling and numbness I mentioned earlier ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The tingling and numbness I mentioned earlier ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I've started experiencing episodes of dizzines...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 243 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Initialize a variable to keep track of the maximum sentence length\nmax_sentence_length = 0\n\n# Iterate through each record in the dataset\nfor index, row in df.iterrows():\n    # Extract the week description from the current record\n    week_description = row['week_description']\n    \n    # Compute the length of the sentence by splitting it into words\n    sentence_length = len(week_description.split())\n    \n    # Update the maximum sentence length if the current sentence is longer\n    max_sentence_length = max(max_sentence_length, sentence_length)\n\n# Print the maximum sentence length\nprint(f\"Maximum sentence length: {max_sentence_length}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:13:58.334659Z","iopub.execute_input":"2023-10-01T21:13:58.335067Z","iopub.status.idle":"2023-10-01T21:13:59.274443Z","shell.execute_reply.started":"2023-10-01T21:13:58.335034Z","shell.execute_reply":"2023-10-01T21:13:59.273392Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Maximum sentence length: 89\n","output_type":"stream"}]},{"cell_type":"code","source":"# Report the total number of records in the dataset\nprint('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n\n# Separate the text data and labels for further processing\n# Extract the 'week_description' column as the text data\ntexts = df['week_description'].values\n\n# Drop the 'week_description' column and use the remaining columns as labels\nlabels = df.drop('week_description', axis=1).values","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:13:59.276112Z","iopub.execute_input":"2023-10-01T21:13:59.276877Z","iopub.status.idle":"2023-10-01T21:13:59.300436Z","shell.execute_reply.started":"2023-10-01T21:13:59.276820Z","shell.execute_reply":"2023-10-01T21:13:59.299449Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Number of training sentences: 16,935\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Text Tokenization and Input Preparation\n\nIn this part of the code, we focus on preparing our text data for the BERT model by tokenizing the text, padding and truncating the token sequences, and creating attention masks to indicate which tokens are meaningful and which are padding.","metadata":{}},{"cell_type":"code","source":"# Instantiate a BlueBERT tokenizer to convert text to token IDs\ntokenizer = BertTokenizer.from_pretrained('bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12')\n\n# Step 1: Tokenize Text\n# Tokenize the text data, adding the special tokens [CLS] and [SEP] as required by BERT\ninput_ids = [tokenizer.encode(text, add_special_tokens=True) for text in texts]\n\n# Step 2: Padding and Truncating\n# Ensure that all sequences are of the same length by padding and truncating\n# We choose a maximum length of 128 tokens for this purpose\ninput_ids = pad_sequences(\n    input_ids, \n    maxlen=128, \n    dtype=\"long\", \n    value=0,  # Value used for padding\n    truncating=\"post\",  # Truncate sequences from the end if necessary\n    padding=\"post\"  # Pad sequences at the end if necessary\n)\n\n# Step 3: Create Attention Masks\n# Create attention masks to differentiate actual tokens from padding\n# A mask value of 1 indicates a real token, while a value of 0 indicates padding\nattention_masks = []\nfor seq in input_ids:\n    # Create a mask for the current sequence\n    seq_mask = [float(i > 0) for i in seq]  # i > 0 checks whether the token ID is not padding (0)\n    attention_masks.append(seq_mask)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:13:59.301796Z","iopub.execute_input":"2023-10-01T21:13:59.302106Z","iopub.status.idle":"2023-10-01T21:14:17.557593Z","shell.execute_reply.started":"2023-10-01T21:13:59.302077Z","shell.execute_reply":"2023-10-01T21:14:17.556630Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3553fd97793e46139060428925b39ff0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4926cf0cc99c4a5eaf912a074258f4f1"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Splitting and DataLoader Preparation\n\nIn this section, we split the input data and labels into training, validation, and test sets to prepare them for model training, validation, and testing. Additionally, we convert the data into PyTorch tensors and organize them into DataLoaders for efficient batch processing during training and evaluation.","metadata":{}},{"cell_type":"code","source":"# Step 1: Splitting the Data\n# Split the input data and labels into training and temporary sets (for further splitting)\ntrain_inputs, temp_inputs, train_labels, temp_labels = train_test_split(\n    input_ids, labels, \n    random_state=42,  # Ensures reproducibility\n    test_size=0.2  # Specifies the proportion of the data to include in the test set\n)\n\n# Split the temporary sets into validation and test sets\nval_inputs, test_inputs, val_labels, test_labels = train_test_split(\n    temp_inputs, temp_labels, \n    random_state=42,  # Ensures reproducibility\n    test_size=0.5  # Specifies the proportion of the data to include in the test set\n)\n\n# Also split the attention masks in a similar fashion\ntrain_masks, temp_masks, _, _ = train_test_split(attention_masks, labels, random_state=42, test_size=0.2)\nval_masks, test_masks, _, _ = train_test_split(temp_masks, temp_labels, random_state=42, test_size=0.5)\n\n# Step 2: Converting Data to PyTorch Tensors\n# Convert all inputs, masks, and labels to PyTorch tensors as required for training in PyTorch\ntrain_inputs = torch.tensor(train_inputs)\nval_inputs = torch.tensor(val_inputs)\ntest_inputs = torch.tensor(test_inputs)\n\ntrain_masks = torch.tensor(train_masks)\nval_masks = torch.tensor(val_masks)\ntest_masks = torch.tensor(test_masks)\n\ntrain_labels = torch.tensor(train_labels)\nval_labels = torch.tensor(val_labels)\ntest_labels = torch.tensor(test_labels)\n\n# Step 3: Creating DataLoaders\n# Organize the data into DataLoaders for efficient batch processing during training and evaluation\n# Set the batch size to 32\nbatch_size = 32\n\n# Create DataLoader for the training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_dataloader = DataLoader(train_data, batch_size=batch_size)\n\n# Create DataLoader for the validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_dataloader = DataLoader(val_data, batch_size=batch_size)\n\n# Create DataLoader for the test set\ntest_data = TensorDataset(test_inputs, test_masks, test_labels)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:14:17.558741Z","iopub.execute_input":"2023-10-01T21:14:17.559075Z","iopub.status.idle":"2023-10-01T21:14:18.056569Z","shell.execute_reply.started":"2023-10-01T21:14:17.559041Z","shell.execute_reply":"2023-10-01T21:14:18.055599Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Model Initialization and Device Preparation\n\nIn this section, we initialize the BERT model for sequence classification and a linear classifier. We also prepare the computing device (CPU or GPU) for training and evaluation.","metadata":{}},{"cell_type":"code","source":"# Step 1: Initialize BlueBERT Model\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12\", \n    num_labels=train_labels.shape[1]  # The number of output labels equals the number of columns in train_labels\n)\n\n# Step 2: Initialize Linear Classifier\n# Initialize a linear classifier to be trained alongside the BERT model\n# The input and output dimensions are both equal to the number of labels\nclassifier = torch.nn.Linear(\n    train_labels.shape[1], \n    train_labels.shape[1]\n)\n\n# Step 3: Prepare Computing Device\n# Determine the computing device (CPU or GPU) and send the model and classifier to this device\ndevice = torch.device(\n    \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU (cuda) if available, otherwise fall back to CPU\n)\n\n# Send the model and classifier to the chosen device\nmodel.to(device)\nclassifier.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:14:18.057920Z","iopub.execute_input":"2023-10-01T21:14:18.058838Z","iopub.status.idle":"2023-10-01T21:14:26.980724Z","shell.execute_reply.started":"2023-10-01T21:14:18.058807Z","shell.execute_reply":"2023-10-01T21:14:26.979722Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/441M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a946353c212e4ac7b4165b1fdd2890e3"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12 and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Linear(in_features=242, out_features=242, bias=True)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Calculating Class Weights\n\nIn this section, we calculate the class weights which are useful to handle imbalanced datasets during training.","metadata":{}},{"cell_type":"code","source":"# Step 1: Count Positive and Negative Samples\n# Count the number of positive samples for each label by summing up the training labels along the column axis\nn_pos = train_labels.sum(axis=0)\n# Uncomment the line below to print the count of positive samples for debugging\n# print(n_pos)\n\n# Calculate the number of negative samples for each label by subtracting the count of positive samples from the total count\nn_neg = len(train_labels) - n_pos\n# Uncomment the line below to print the count of negative samples for debugging\n# print(n_neg)\n\n# Step 2: Calculate Positive Class Weights\n# The positive class weights are calculated as the ratio of negative samples to positive samples for each label\n# Adding a small constant (1e-5) to avoid division by zero\npos_weights = (n_neg + 1e-5) / (n_pos + 1e-5)\n\n# Uncomment the line below to print the calculated positive class weights for debugging\n# print(f\"Positive Weights: {pos_weights}\")\n\n# Step 3: Convert to Tensor and Send to Device\n# Convert the numpy array of positive class weights to a PyTorch tensor\n# Then send the tensor to the chosen computing device (CPU or GPU)\npos_weights = torch.tensor(pos_weights).to(device)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:14:26.984376Z","iopub.execute_input":"2023-10-01T21:14:26.984666Z","iopub.status.idle":"2023-10-01T21:14:27.007751Z","shell.execute_reply.started":"2023-10-01T21:14:26.984641Z","shell.execute_reply":"2023-10-01T21:14:27.006836Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_67/1551397999.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  pos_weights = torch.tensor(pos_weights).to(device)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Optimizer, Learning Rate Scheduler, and Loss Function Initialization\n\nIn this section, we initialize the optimizer, learning rate scheduler, and the loss function which are essential components for training the neural network.","metadata":{}},{"cell_type":"code","source":"# Step 1: Initialize Optimizer\n# We use the AdamW optimizer which is an extension of Adam optimized for training deep neural networks\n# We include the parameters of both the BERT model and the linear classifier in the optimizer\noptimizer = AdamW(\n    list(model.parameters()) + list(classifier.parameters()),  # Combine the parameters of model and classifier\n    lr=0.0001  # Set the learning rate\n)\n\n# Step 2: Initialize Learning Rate Scheduler\n# The StepLR scheduler adjusts the learning rate at regular intervals for better convergence\nscheduler = StepLR(\n    optimizer, \n    step_size=10,  # Decrease the learning rate every 10 epochs\n    gamma=0.7  # Multiplicative factor to decrease the learning rate\n)\n\n# Step 3: Initialize Loss Function\n# The pos_weight argument helps handle imbalanced datasets by scaling the loss for positive samples\ncriterion = BCEWithLogitsLoss(\n    pos_weight=pos_weights  # Set the positive weights to handle class imbalance\n)\n\n# Alternative Loss Function\n# The Focal Loss is another alternative for handling imbalanced datasets\n# Uncomment the following lines to use Focal Loss instead of BCEWithLogitsLoss\n# criterion = FocalLoss(\n#     alpha=1, \n#     gamma=2, \n#     logits=True, \n#     reduce=True\n# )\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:14:27.009121Z","iopub.execute_input":"2023-10-01T21:14:27.009689Z","iopub.status.idle":"2023-10-01T21:14:27.016787Z","shell.execute_reply.started":"2023-10-01T21:14:27.009658Z","shell.execute_reply":"2023-10-01T21:14:27.015858Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Utility Functions for Printing Metrics\n\nIn this section, we introduce two utility functions for pretty-printing the metrics during training and evaluation.","metadata":{}},{"cell_type":"code","source":"def pretty_print_metrics(title, metrics):\n    \"\"\"\n    Prints the metrics in a tabular format.\n    \n    :param title: (str) Title to be displayed above the table.\n    :param metrics: (dict) Dictionary containing metric names as keys and their values.\n    \"\"\"\n    \n    # Display the title for the metrics\n    print(f\"\\n{title}\")\n    \n    # Initialize the table\n    table = PrettyTable()\n    \n    # Set the column names for the table\n    table.field_names = [\"Metric\", \"Value\"]\n    \n    # Add rows to the table using metrics data\n    for metric, value in metrics.items():\n        table.add_row([metric, f\"{value:.4f}\"])\n    \n    # Print the table\n    print(table)\n\ndef pretty_print_epoch(epoch, train_loss, val_loss):\n    \"\"\"\n    Prints the training and validation loss for each epoch in a tabular format.\n    \n    :param epoch: (int) Current epoch number.\n    :param train_loss: (float) Training loss for the current epoch.\n    :param val_loss: (float) Validation loss for the current epoch.\n    \"\"\"\n    \n    # Display epoch information\n    print(f\"\\n{'='*40}\")\n    print(f\"Epoch {epoch}\")\n    print(f\"{'='*40}\")\n    \n    # Initialize the table\n    table = PrettyTable()\n    \n    # Set the column names for the table\n    table.field_names = [\"Data Type\", \"Loss\"]\n    \n    # Add rows to the table for training and validation data\n    table.add_row([\"Training\", f\"{train_loss:.4f}\"])\n    table.add_row([\"Validation\", f\"{val_loss:.4f}\"])\n    \n    # Print the table\n    print(table)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:14:27.018053Z","iopub.execute_input":"2023-10-01T21:14:27.019005Z","iopub.status.idle":"2023-10-01T21:14:27.034025Z","shell.execute_reply.started":"2023-10-01T21:14:27.018919Z","shell.execute_reply":"2023-10-01T21:14:27.033069Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Updating Label Data Types\n\nIn this section, we are updating the data types of the label tensors to torch.float32. This data type conversion is essential for ensuring compatibility with PyTorch operations, when using loss functions like BCEWithLogitsLoss.","metadata":{}},{"cell_type":"code","source":"# Converting the data type of training labels to float32\ntrain_labels = torch.tensor(train_labels, dtype=torch.float32)\n\n# Converting the data type of validation labels to float32\nval_labels = torch.tensor(val_labels, dtype=torch.float32)\n\n# Converting the data type of testing labels to float32\ntest_labels = torch.tensor(test_labels, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:14:27.035420Z","iopub.execute_input":"2023-10-01T21:14:27.036245Z","iopub.status.idle":"2023-10-01T21:14:27.062478Z","shell.execute_reply.started":"2023-10-01T21:14:27.036217Z","shell.execute_reply":"2023-10-01T21:14:27.061643Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_67/3817560295.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  train_labels = torch.tensor(train_labels, dtype=torch.float32)\n/tmp/ipykernel_67/3817560295.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  val_labels = torch.tensor(val_labels, dtype=torch.float32)\n/tmp/ipykernel_67/3817560295.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  test_labels = torch.tensor(test_labels, dtype=torch.float32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training and Validation Loop\n\nIn this section, we set up and run the training and validation loop. We initialize arrays to keep track of training and validation losses over the epochs. In each epoch, we perform a forward and backward pass on the training data, update the model parameters, and compute the average training loss. Then, we evaluate the model on the validation set, compute the average validation loss, and evaluate several performance metrics.","metadata":{}},{"cell_type":"code","source":"# Arrays to store training and validation loss values across epochs\ntrain_loss_values = []\nval_loss_values = []\n\n# Training loop across epochs\nfor epoch in range(16):\n    model.train()  # Set model to training mode\n    \n    avg_train_loss = 0  # Initialize average training loss for the epoch\n    avg_val_loss = 0  # Initialize average validation loss for the epoch\n    \n    # Loop over batches of training data\n    for i, batch in enumerate(train_dataloader):\n        \n        # Send input data and labels to the device\n        inputs, masks, labels = tuple(t.to(device) for t in batch)\n        \n        # Ensure labels are float for loss computation\n        labels = labels.float()\n\n        # Forward pass: compute predictions\n        outputs = model(inputs, attention_mask=masks)\n        logits_from_model = outputs.logits\n        logits = classifier(logits_from_model)\n        \n        # Compute loss\n        loss = criterion(logits, labels)\n        \n        # Perform backward pass to compute gradients\n        loss.backward()\n        \n        # Gradient clipping to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        # Update model parameters\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        # Update average training loss\n        avg_train_loss += loss.item() / len(train_dataloader)\n        \n        # Print training loss every 100 batches\n        if i % 100 == 0:\n            print(f\"Epoch: {epoch}, Batch: {i}, Training Loss: {loss.item()}\")\n        \n    # Store average training loss for the epoch\n    train_loss_values.append(avg_train_loss)\n\n    # Validation: Evaluate model on validation data\n    model.eval()  # Set model to evaluation mode\n    val_preds = []\n    val_true = []\n    with torch.no_grad():  # No gradient computation\n        for batch in val_dataloader:\n            \n            # Send input data and labels to the device\n            inputs, masks, labels = tuple(t.to(device) for t in batch)\n            \n            # Forward pass: compute predictions\n            outputs = model(inputs, attention_mask=masks)\n            logits_from_model = outputs.logits\n            logits = classifier(logits_from_model)\n            \n            # Store predictions and true labels for later evaluation\n            preds = torch.sigmoid(logits).cpu().numpy()\n            val_preds.extend(preds)\n            val_true.extend(labels.cpu().numpy())\n            \n            # Update average validation loss\n            avg_val_loss += loss.item() / len(val_dataloader)\n        val_loss_values.append(avg_val_loss)\n\n    # Convert prediction probabilities to binary predictions\n    val_preds = np.array(val_preds) >= 0.7\n    val_true = np.array(val_true)\n    \n    # Compute validation accuracy\n    val_accuracy = accuracy_score(val_true, val_preds)\n    \n    # Print summary for the epoch\n    pretty_print_epoch(epoch, avg_train_loss, avg_val_loss)\n    \n    # Compute and print several performance metrics on validation data\n    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(val_true, val_preds, average='micro')\n    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(val_true, val_preds, average='macro')\n    hamming = hamming_loss(val_true, val_preds)\n    jaccard = jaccard_score(val_true, val_preds, average='samples')\n    logloss = log_loss(val_true, val_preds)\n    \n    val_metrics = {\n        'Validation Accuracy': val_accuracy,\n        'Micro-average Precision': precision_micro,\n        'Micro-average Recall': recall_micro,\n        'Micro-average F1': f1_micro,\n        'Macro-average Precision': precision_macro,\n        'Macro-average Recall': recall_macro,\n        'Macro-average F1': f1_macro,\n        'Hamming Loss': hamming,\n        'Jaccard Similarity': jaccard,\n        'Log Loss': logloss\n    }\n\n    pretty_print_metrics('Validation Metrics', val_metrics)  # Use PrettyTable to print validation metrics\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-01T21:14:27.063872Z","iopub.execute_input":"2023-10-01T21:14:27.064158Z","iopub.status.idle":"2023-10-01T21:56:49.690313Z","shell.execute_reply.started":"2023-10-01T21:14:27.064130Z","shell.execute_reply":"2023-10-01T21:56:49.689375Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Epoch: 0, Batch: 0, Training Loss: 1.335249900817871\nEpoch: 0, Batch: 100, Training Loss: 1.079715371131897\nEpoch: 0, Batch: 200, Training Loss: 0.8810611963272095\nEpoch: 0, Batch: 300, Training Loss: 0.5951901078224182\nEpoch: 0, Batch: 400, Training Loss: 0.41555601358413696\n\n========================================\nEpoch 0\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.8494 |\n| Validation | 0.3858 |\n+------------+--------+\n\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.0130 |\n| Micro-average Precision | 0.0904 |\n|   Micro-average Recall  | 0.9498 |\n|     Micro-average F1    | 0.1650 |\n| Macro-average Precision | 0.2057 |\n|   Macro-average Recall  | 0.9574 |\n|     Macro-average F1    | 0.2952 |\n|       Hamming Loss      | 0.0397 |\n|    Jaccard Similarity   | 0.1412 |\n|         Log Loss        | 3.9597 |\n+-------------------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Batch: 0, Training Loss: 0.39615315198898315\nEpoch: 1, Batch: 100, Training Loss: 0.25932490825653076\nEpoch: 1, Batch: 200, Training Loss: 0.2617379426956177\nEpoch: 1, Batch: 300, Training Loss: 0.22337336838245392\nEpoch: 1, Batch: 400, Training Loss: 0.18857428431510925\n\n========================================\nEpoch 1\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.2685 |\n| Validation | 0.1904 |\n+------------+--------+\n\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.0467 |\n| Micro-average Precision | 0.1401 |\n|   Micro-average Recall  | 0.9687 |\n|     Micro-average F1    | 0.2447 |\n| Macro-average Precision | 0.2670 |\n|   Macro-average Recall  | 0.9692 |\n|     Macro-average F1    | 0.3729 |\n|       Hamming Loss      | 0.0247 |\n|    Jaccard Similarity   | 0.2266 |\n|         Log Loss        | 2.8510 |\n+-------------------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Batch: 0, Training Loss: 0.2085455358028412\nEpoch: 2, Batch: 100, Training Loss: 0.136262908577919\nEpoch: 2, Batch: 200, Training Loss: 0.17005683481693268\nEpoch: 2, Batch: 300, Training Loss: 0.13584375381469727\nEpoch: 2, Batch: 400, Training Loss: 0.14268891513347626\n\n========================================\nEpoch 2\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.1604 |\n| Validation | 0.1207 |\n+------------+--------+\n\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.0851 |\n| Micro-average Precision | 0.1749 |\n|   Micro-average Recall  | 0.9722 |\n|     Micro-average F1    | 0.2964 |\n| Macro-average Precision | 0.3300 |\n|   Macro-average Recall  | 0.9716 |\n|     Macro-average F1    | 0.4412 |\n|       Hamming Loss      | 0.0191 |\n|    Jaccard Similarity   | 0.2935 |\n|         Log Loss        | 2.4768 |\n+-------------------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 3, Batch: 0, Training Loss: 0.14495337009429932\nEpoch: 3, Batch: 100, Training Loss: 0.10020644962787628\nEpoch: 3, Batch: 200, Training Loss: 0.13731689751148224\nEpoch: 3, Batch: 300, Training Loss: 0.10420748591423035\nEpoch: 3, Batch: 400, Training Loss: 0.11131082475185394\n\n========================================\nEpoch 3\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.1215 |\n| Validation | 0.0914 |\n+------------+--------+\n\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.0827 |\n| Micro-average Precision | 0.1912 |\n|   Micro-average Recall  | 0.9693 |\n|     Micro-average F1    | 0.3194 |\n| Macro-average Precision | 0.3571 |\n|   Macro-average Recall  | 0.9695 |\n|     Macro-average F1    | 0.4709 |\n|       Hamming Loss      | 0.0171 |\n|    Jaccard Similarity   | 0.3113 |\n|         Log Loss        | 2.4958 |\n+-------------------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 4, Batch: 0, Training Loss: 0.12151769548654556\nEpoch: 4, Batch: 100, Training Loss: 0.0843575969338417\nEpoch: 4, Batch: 200, Training Loss: 0.10424919426441193\nEpoch: 4, Batch: 300, Training Loss: 0.08583641052246094\nEpoch: 4, Batch: 400, Training Loss: 0.08221715688705444\n\n========================================\nEpoch 4\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.1024 |\n| Validation | 0.0881 |\n+------------+--------+\n\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.0945 |\n| Micro-average Precision | 0.2007 |\n|   Micro-average Recall  | 0.9711 |\n|     Micro-average F1    | 0.3326 |\n| Macro-average Precision | 0.3913 |\n|   Macro-average Recall  | 0.9709 |\n|     Macro-average F1    | 0.5001 |\n|       Hamming Loss      | 0.0161 |\n|    Jaccard Similarity   | 0.3302 |\n|         Log Loss        | 2.3774 |\n+-------------------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 5, Batch: 0, Training Loss: 0.10868018120527267\nEpoch: 5, Batch: 100, Training Loss: 0.07658599317073822\nEpoch: 5, Batch: 200, Training Loss: 0.09986186772584915\nEpoch: 5, Batch: 300, Training Loss: 0.08071956038475037\nEpoch: 5, Batch: 400, Training Loss: 0.07825011014938354\n\n========================================\nEpoch 5\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.0913 |\n| Validation | 0.0743 |\n+------------+--------+\n\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.1051 |\n| Micro-average Precision | 0.2108 |\n|   Micro-average Recall  | 0.9722 |\n|     Micro-average F1    | 0.3465 |\n| Macro-average Precision | 0.4057 |\n|   Macro-average Recall  | 0.9716 |\n|     Macro-average F1    | 0.5159 |\n|       Hamming Loss      | 0.0152 |\n|    Jaccard Similarity   | 0.3432 |\n|         Log Loss        | 2.2918 |\n+-------------------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 6, Batch: 0, Training Loss: 0.09732561558485031\nEpoch: 6, Batch: 100, Training Loss: 0.07282290607690811\nEpoch: 6, Batch: 200, Training Loss: 0.08661873638629913\nEpoch: 6, Batch: 300, Training Loss: 0.0767013356089592\nEpoch: 6, Batch: 400, Training Loss: 0.0697607472538948\n\n========================================\nEpoch 6\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.0861 |\n| Validation | 0.0684 |\n+------------+--------+\n\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.0975 |\n| Micro-average Precision | 0.2092 |\n|   Micro-average Recall  | 0.9693 |\n|     Micro-average F1    | 0.3441 |\n| Macro-average Precision | 0.3939 |\n|   Macro-average Recall  | 0.9710 |\n|     Macro-average F1    | 0.5059 |\n|       Hamming Loss      | 0.0153 |\n|    Jaccard Similarity   | 0.3339 |\n|         Log Loss        | 2.4151 |\n+-------------------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 7, Batch: 0, Training Loss: 0.09642085433006287\nEpoch: 7, Batch: 100, Training Loss: 0.06994263082742691\nEpoch: 7, Batch: 200, Training Loss: 0.08037038892507553\nEpoch: 7, Batch: 300, Training Loss: 0.06126704812049866\nEpoch: 7, Batch: 400, Training Loss: 0.07903864979743958\n\n========================================\nEpoch 7\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.0826 |\n| Validation | 0.0643 |\n+------------+--------+\n\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.1063 |\n| Micro-average Precision | 0.2212 |\n|   Micro-average Recall  | 0.9652 |\n|     Micro-average F1    | 0.3600 |\n| Macro-average Precision | 0.4172 |\n|   Macro-average Recall  | 0.9671 |\n|     Macro-average F1    | 0.5280 |\n|       Hamming Loss      | 0.0142 |\n|    Jaccard Similarity   | 0.3530 |\n|         Log Loss        | 2.4794 |\n+-------------------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 8, Batch: 0, Training Loss: 0.09892499446868896\nEpoch: 8, Batch: 100, Training Loss: 0.06560958921909332\nEpoch: 8, Batch: 200, Training Loss: 0.07824496179819107\nEpoch: 8, Batch: 300, Training Loss: 0.06278054416179657\nEpoch: 8, Batch: 400, Training Loss: 0.08772797882556915\n\n========================================\nEpoch 8\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.0792 |\n| Validation | 0.0565 |\n+------------+--------+\n\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.1146 |\n| Micro-average Precision | 0.2283 |\n|   Micro-average Recall  | 0.9640 |\n|     Micro-average F1    | 0.3691 |\n| Macro-average Precision | 0.4256 |\n|   Macro-average Recall  | 0.9670 |\n|     Macro-average F1    | 0.5374 |\n|       Hamming Loss      | 0.0136 |\n|    Jaccard Similarity   | 0.3638 |\n|         Log Loss        | 2.4875 |\n+-------------------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 9, Batch: 0, Training Loss: 0.08790630847215652\nEpoch: 9, Batch: 100, Training Loss: 0.06372501701116562\nEpoch: 9, Batch: 200, Training Loss: 0.07329169660806656\nEpoch: 9, Batch: 300, Training Loss: 0.06081893667578697\nEpoch: 9, Batch: 400, Training Loss: 0.06767849624156952\n\n========================================\nEpoch 9\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.0749 |\n| Validation | 0.0652 |\n+------------+--------+\n\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.1294 |\n| Micro-average Precision | 0.2349 |\n|   Micro-average Recall  | 0.9652 |\n|     Micro-average F1    | 0.3779 |\n| Macro-average Precision | 0.4398 |\n|   Macro-average Recall  | 0.9674 |\n|     Macro-average F1    | 0.5500 |\n|       Hamming Loss      | 0.0131 |\n|    Jaccard Similarity   | 0.3745 |\n|         Log Loss        | 2.4353 |\n+-------------------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 10, Batch: 0, Training Loss: 0.08680222928524017\nEpoch: 10, Batch: 100, Training Loss: 0.0625317171216011\nEpoch: 10, Batch: 200, Training Loss: 0.07346738874912262\nEpoch: 10, Batch: 300, Training Loss: 0.08373160660266876\nEpoch: 10, Batch: 400, Training Loss: 0.07716929912567139\n\n========================================\nEpoch 10\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.0730 |\n| Validation | 0.0730 |\n+------------+--------+\n\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.1288 |\n| Micro-average Precision | 0.2321 |\n|   Micro-average Recall  | 0.9681 |\n|     Micro-average F1    | 0.3745 |\n| Macro-average Precision | 0.4406 |\n|   Macro-average Recall  | 0.9691 |\n|     Macro-average F1    | 0.5506 |\n|       Hamming Loss      | 0.0134 |\n|    Jaccard Similarity   | 0.3729 |\n|         Log Loss        | 2.3412 |\n+-------------------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 11, Batch: 0, Training Loss: 0.08683343231678009\nEpoch: 11, Batch: 100, Training Loss: 0.06102440133690834\nEpoch: 11, Batch: 200, Training Loss: 0.0721881315112114\nEpoch: 11, Batch: 300, Training Loss: 0.056841831654310226\nEpoch: 11, Batch: 400, Training Loss: 0.07062205672264099\n\n========================================\nEpoch 11\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.0703 |\n| Validation | 0.0530 |\n+------------+--------+\n\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.1282 |\n| Micro-average Precision | 0.2390 |\n|   Micro-average Recall  | 0.9657 |\n|     Micro-average F1    | 0.3831 |\n| Macro-average Precision | 0.4443 |\n|   Macro-average Recall  | 0.9654 |\n|     Macro-average F1    | 0.5542 |\n|       Hamming Loss      | 0.0129 |\n|    Jaccard Similarity   | 0.3772 |\n|         Log Loss        | 2.4004 |\n+-------------------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 12, Batch: 0, Training Loss: 0.08152706921100616\nEpoch: 12, Batch: 100, Training Loss: 0.05793904513120651\nEpoch: 12, Batch: 200, Training Loss: 0.06820719689130783\nEpoch: 12, Batch: 300, Training Loss: 0.056295245885849\nEpoch: 12, Batch: 400, Training Loss: 0.06972803175449371\n\n========================================\nEpoch 12\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.0746 |\n| Validation | 0.0607 |\n+------------+--------+\n\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.1075 |\n| Micro-average Precision | 0.2345 |\n|   Micro-average Recall  | 0.9604 |\n|     Micro-average F1    | 0.3770 |\n| Macro-average Precision | 0.4213 |\n|   Macro-average Recall  | 0.9595 |\n|     Macro-average F1    | 0.5338 |\n|       Hamming Loss      | 0.0131 |\n|    Jaccard Similarity   | 0.3619 |\n|         Log Loss        | 2.6169 |\n+-------------------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 13, Batch: 0, Training Loss: 0.08989276736974716\nEpoch: 13, Batch: 100, Training Loss: 0.06292172521352768\nEpoch: 13, Batch: 200, Training Loss: 0.07517334818840027\nEpoch: 13, Batch: 300, Training Loss: 0.054033536463975906\nEpoch: 13, Batch: 400, Training Loss: 0.06467854231595993\n\n========================================\nEpoch 13\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.0719 |\n| Validation | 0.0667 |\n+------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.1370 |\n| Micro-average Precision | 0.2406 |\n|   Micro-average Recall  | 0.9669 |\n|     Micro-average F1    | 0.3854 |\n| Macro-average Precision | 0.4384 |\n|   Macro-average Recall  | 0.9684 |\n|     Macro-average F1    | 0.5497 |\n|       Hamming Loss      | 0.0127 |\n|    Jaccard Similarity   | 0.3840 |\n|         Log Loss        | 2.3499 |\n+-------------------------+--------+\nEpoch: 14, Batch: 0, Training Loss: 0.09875725954771042\nEpoch: 14, Batch: 100, Training Loss: 0.07911080867052078\nEpoch: 14, Batch: 200, Training Loss: 0.06528807431459427\nEpoch: 14, Batch: 300, Training Loss: 0.06206272169947624\nEpoch: 14, Batch: 400, Training Loss: 0.0853206068277359\n\n========================================\nEpoch 14\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.0699 |\n| Validation | 0.0543 |\n+------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.1435 |\n| Micro-average Precision | 0.2479 |\n|   Micro-average Recall  | 0.9622 |\n|     Micro-average F1    | 0.3942 |\n| Macro-average Precision | 0.4444 |\n|   Macro-average Recall  | 0.9648 |\n|     Macro-average F1    | 0.5568 |\n|       Hamming Loss      | 0.0122 |\n|    Jaccard Similarity   | 0.3898 |\n|         Log Loss        | 2.4890 |\n+-------------------------+--------+\nEpoch: 15, Batch: 0, Training Loss: 0.08084563165903091\nEpoch: 15, Batch: 100, Training Loss: 0.060496147722005844\nEpoch: 15, Batch: 200, Training Loss: 0.06891963630914688\nEpoch: 15, Batch: 300, Training Loss: 0.055781859904527664\nEpoch: 15, Batch: 400, Training Loss: 0.0718471109867096\n\n========================================\nEpoch 15\n========================================\n+------------+--------+\n| Data Type  |  Loss  |\n+------------+--------+\n|  Training  | 0.0664 |\n| Validation | 0.0471 |\n+------------+--------+\n\nValidation Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|   Validation Accuracy   | 0.1453 |\n| Micro-average Precision | 0.2501 |\n|   Micro-average Recall  | 0.9616 |\n|     Micro-average F1    | 0.3969 |\n| Macro-average Precision | 0.4543 |\n|   Macro-average Recall  | 0.9629 |\n|     Macro-average F1    | 0.5646 |\n|       Hamming Loss      | 0.0121 |\n|    Jaccard Similarity   | 0.3945 |\n|         Log Loss        | 2.4820 |\n+-------------------------+--------+\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation on Test Set\n\nThis section presents the evaluation of the model on the test set. The model's performance is assessed across various metrics including loss, accuracy, precision, recall, F1-score, Hamming Loss, Jaccard Similarity, and Log Loss. The evaluation is carried out batch-wise, and the predictions along with labels are stored for metric computation.","metadata":{}},{"cell_type":"code","source":"# Set the model to evaluation mode\nmodel.eval()\n\n# Initialize variables to keep track of total loss and correct predictions\ntotal_loss = 0\ntotal_correct = 0\n\n# Lists to store all predictions and labels\nall_preds = []\nall_labels = []\n\n# Loop over batches in the test dataloader\nfor i, batch in enumerate(test_dataloader):\n    \n    # Unpack the batch and move tensors to the appropriate device\n    inputs, masks, labels = tuple(t.to(device) for t in batch)\n    \n    # Ensure labels are float for loss computation\n    labels = labels.float()\n    \n    # No gradient computation in evaluation phase\n    with torch.no_grad():\n        # Forward pass: Compute predictions\n        outputs = model(inputs, attention_mask=masks)\n        logits_from_model = outputs.logits\n        logits = classifier(logits_from_model)\n        \n        # Compute loss\n        loss = criterion(logits, labels)\n        \n        # Accumulate loss\n        total_loss += loss.item()\n        \n        # Convert logits to probabilities and store predictions and labels\n        preds = torch.sigmoid(logits).cpu().numpy()\n        all_preds.extend(preds)\n        all_labels.extend(labels.cpu().numpy())\n\n# Convert list of predictions and labels to NumPy arrays for easier manipulation\nall_preds_np = np.array(all_preds)\nall_labels_np = np.array(all_labels)\n\n# Binarize predictions based on threshold\nbinary_preds = all_preds_np >= 0.7\n\n# Calculate Metrics\nprecision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(all_labels_np, binary_preds, average='micro')\nprecision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(all_labels_np, binary_preds, average='macro')\nhamming = hamming_loss(all_labels_np, binary_preds)\njaccard = jaccard_score(all_labels_np, binary_preds, average='samples')\nlogloss = log_loss(all_labels_np, all_preds_np)\naccuracy = accuracy_score(all_labels_np, binary_preds)\n\n# Organize metrics in a dictionary and print using pretty_print_metrics function\ntest_metrics = {\n    'Test Loss': total_loss / len(test_dataloader),\n    'Test Accuracy': accuracy,\n    'Micro-Average Precision': precision_micro,\n    'Micro-Average Recall': recall_micro,\n    'Micro-Average F1-score': f1_micro,\n    'Macro-Average Precision': precision_macro,\n    'Macro-Average Recall': recall_macro,\n    'Macro-Average F1-score': f1_macro,\n    'Hamming Loss': hamming,\n    'Jaccard Similarity': jaccard,\n    'Log Loss': logloss\n}\n\n# Use PrettyTable to display test metrics\npretty_print_metrics('Test Metrics', test_metrics)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:56:49.692090Z","iopub.execute_input":"2023-10-01T21:56:49.692433Z","iopub.status.idle":"2023-10-01T21:56:55.780955Z","shell.execute_reply.started":"2023-10-01T21:56:49.692402Z","shell.execute_reply":"2023-10-01T21:56:55.779890Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\nTest Metrics\n+-------------------------+--------+\n|          Metric         | Value  |\n+-------------------------+--------+\n|        Test Loss        | 0.1247 |\n|      Test Accuracy      | 0.1387 |\n| Micro-Average Precision | 0.2486 |\n|   Micro-Average Recall  | 0.9628 |\n|  Micro-Average F1-score | 0.3952 |\n| Macro-Average Precision | 0.4579 |\n|   Macro-Average Recall  | 0.9745 |\n|  Macro-Average F1-score | 0.5660 |\n|       Hamming Loss      | 0.0122 |\n|    Jaccard Similarity   | 0.3900 |\n|         Log Loss        | 1.4640 |\n+-------------------------+--------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Plotting Training and Validation Loss\n\nIn this section, we use Plotly to create a plot that depicts the training and validation loss over epochs. Each point on the plot represents the loss at the end of an epoch, for both training and validation phases. This visualization helps in understanding how the model is learning and whether it's overfitting (if the validation loss starts increasing).","metadata":{}},{"cell_type":"code","source":"# Create a figure using Plotly's go.Figure()\nfig = go.Figure()\n\n# Add a trace for training loss values over epochs\nfig.add_trace(\n    go.Scatter(\n        x=list(range(len(train_loss_values))),  # X-axis: Epoch number\n        y=train_loss_values,  # Y-axis: Training loss value\n        mode='lines+markers',  # Mode: Lines + Markers for individual data points\n        name='Training Loss'  # Trace name\n    )\n)\n\n# Add a trace for validation loss values over epochs\nfig.add_trace(\n    go.Scatter(\n        x=list(range(len(val_loss_values))),  # X-axis: Epoch number\n        y=val_loss_values,  # Y-axis: Validation loss value\n        mode='lines+markers',  # Mode: Lines + Markers for individual data points\n        name='Validation Loss'  # Trace name\n    )\n)\n\n# Update the layout of the figure to include title and axis labels\nfig.update_layout(\n    title='Training and Validation Loss',  # Title of the plot\n    xaxis_title='Epoch',  # X-axis label\n    yaxis_title='Loss'  # Y-axis label\n)\n\n# Display the figure\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:56:55.782531Z","iopub.execute_input":"2023-10-01T21:56:55.782914Z","iopub.status.idle":"2023-10-01T21:56:56.103518Z","shell.execute_reply.started":"2023-10-01T21:56:55.782879Z","shell.execute_reply":"2023-10-01T21:56:56.102560Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"25433a04-c524-46f5-9b58-519b0d36f0e4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"25433a04-c524-46f5-9b58-519b0d36f0e4\")) {                    Plotly.newPlot(                        \"25433a04-c524-46f5-9b58-519b0d36f0e4\",                        [{\"mode\":\"lines+markers\",\"name\":\"Training Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],\"y\":[0.849376760212318,0.26848643680788425,0.1603874579810027,0.12146123194680473,0.10236509759330518,0.09127793643834463,0.0861465848349738,0.08261205299915564,0.0792143825476743,0.07494922997078529,0.07296157116948994,0.07034647695825623,0.07463634899764696,0.07187984358378739,0.06991440921424427,0.06641123352466892],\"type\":\"scatter\"},{\"mode\":\"lines+markers\",\"name\":\"Validation Loss\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],\"y\":[0.3857938945293424,0.19042615592479717,0.12066106498241438,0.09143432974815364,0.08806931972503662,0.07433489710092549,0.06843704730272289,0.06431960314512247,0.05645650625228885,0.06516589969396597,0.07302305847406386,0.05297494307160381,0.06073229387402536,0.06665437668561938,0.05431329086422923,0.0470896847546101],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Training and Validation Loss\"},\"xaxis\":{\"title\":{\"text\":\"Epoch\"}},\"yaxis\":{\"title\":{\"text\":\"Loss\"}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('25433a04-c524-46f5-9b58-519b0d36f0e4');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Predicting on Sample Texts\n\nIn this section, we create a function called predict_sample_texts to preprocess the provided text samples, run them through the model for predictions, and interpret the results to obtain human-readable labels. This function demonstrates how to use a pretrained transformer model and a classifier to predict on new data.","metadata":{}},{"cell_type":"code","source":"# Function to preprocess and predict on new text data\ndef predict_sample_texts(texts, model, classifier, tokenizer, device, threshold=0.7):\n    results = []  # List to store the results\n    \n    # Get the column headers except for the first one as a list\n    headers = df.columns.tolist()[1:]\n    \n    for text in texts:\n        # Step 1: Preprocess\n        # Tokenize the text and add special tokens\n        input_id = tokenizer.encode(text, add_special_tokens=True)\n        # Pad or truncate the token ids to a fixed length\n        input_id = pad_sequences([input_id], maxlen=128, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n        \n        # Create attention mask\n        attention_mask = [[float(i > 0) for i in seq] for seq in input_id]\n        \n        # Convert to PyTorch tensors and move to the given device\n        input_id = torch.tensor(input_id).to(device)\n        attention_mask = torch.tensor(attention_mask).to(device)\n        \n        # Step 2: Model Prediction\n        # Set the model and classifier to evaluation mode\n        model.eval()\n        classifier.eval()\n        \n        # No gradient computation to save memory\n        with torch.no_grad():\n            # Obtain model outputs\n            output = model(input_id, attention_mask=attention_mask)\n            logits_from_model = output.logits\n            logits = classifier(logits_from_model)\n            # Convert logits to probabilities\n            probabilities = torch.sigmoid(logits).cpu().numpy()\n            \n        # Step 3: Interpret the Result\n        # Binarize probabilities based on the threshold\n        binary_output = (probabilities >= threshold).astype(int)\n        \n        # Store the result in a dictionary and append to results list\n        results.append({\n            'text': text,\n            'probabilities': probabilities,\n            'binary_output': binary_output,\n            # Get the predicted labels\n            'predicted_labels': [label for label, output in zip(headers, binary_output[0]) if output == 1]\n        })\n        \n    return results  # Return the results list\n\n# Array of sample texts along with HPO terms for context\n# Headache (HP:0002315)\n# Dry skin (HP:0000958)\n# Nocturia (HP:0000017)\nsample_texts = [\n    \"Still thirsty, still constantly in the bathroom, and now I'm getting headaches that are making it tough to focus. Probably from all the lost fluids. Doctor says my sodium levels are still high. It's like my body's a malfunctioning tap.\",\n    \"I thought things couldn't get worse, but now my skin's as dry as the Sahara. Between this and everything else, I'm feeling pretty miserable.\",\n    \"Been waking up in the middle of the night to use the bathroom. It's affecting my sleep now, and I've started to feel groggy all day long.\"\n]\n\n# Get prediction\nresults = predict_sample_texts(sample_texts, model, classifier, tokenizer, device)\n\n# Output the results\nfor i, result in enumerate(results):\n    print(f\"Sample Text {i+1}: {result['text']}\")\n    print(f\"Predicted Labels: {result['predicted_labels']}\")\n    print(\"=\" * 50)  # Print a separator\n","metadata":{"execution":{"iopub.status.busy":"2023-10-01T21:56:56.105066Z","iopub.execute_input":"2023-10-01T21:56:56.106021Z","iopub.status.idle":"2023-10-01T21:56:56.153773Z","shell.execute_reply.started":"2023-10-01T21:56:56.105987Z","shell.execute_reply":"2023-10-01T21:56:56.152842Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Sample Text 1: Still thirsty, still constantly in the bathroom, and now I'm getting headaches that are making it tough to focus. Probably from all the lost fluids. Doctor says my sodium levels are still high. It's like my body's a malfunctioning tap.\nPredicted Labels: ['HP:0000103', 'HP:0000835', 'HP:0000958', 'HP:0002315']\n==================================================\nSample Text 2: I thought things couldn't get worse, but now my skin's as dry as the Sahara. Between this and everything else, I'm feeling pretty miserable.\nPredicted Labels: ['HP:0000720', 'HP:0000958', 'HP:0001499']\n==================================================\nSample Text 3: Been waking up in the middle of the night to use the bathroom. It's affecting my sleep now, and I've started to feel groggy all day long.\nPredicted Labels: ['HP:0000785']\n==================================================\n","output_type":"stream"}]}]}